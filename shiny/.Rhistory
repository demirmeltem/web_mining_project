all_series[2,26]
all_series[2,25]
all_series[2,24]
all_series[2,2]
all_series[2,2]
1:2
for(i in 1:length(all_series$Show)){
for(j in 3:26){
all_series[i, j] <- as.double(as.character(all_series[i, j]))
}
}
col <- c('Show','Network','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','AveragePerEpisode','Loss_Gain','Drop')
all_series = data.frame()
for(i in 1:length(series_urls)){
URL <- GET(paste(series_urls[i]))
one_series <- readHTMLTable(rawToChar(URL$content))[[1]]
one_series <- one_series[c(-1,-2),]
one_series <- one_series[,c(-1,-4)]
one_series <- one_series[, 1:29]
colnames(one_series) <- col
one_series$Season <- series_seasons[i]
all_series <- rbind(all_series, one_series)
Sys.sleep(1)
}
all_series <- all_series[-which(all_series$Show == "Coach" & all_series$Season == "2015_16"),]
all_series[, 'AveragePerEpisode'] <- as.numeric(as.character(all_series[, 'AveragePerEpisode']))
all_series[, 'Show'] <- as.character(all_series[, 'Show'])
all_series[, 'Network'] <- as.character(all_series[, 'Network'])
all_series[, 'Loss_Gain'] <-as.numeric(as.character(all_series[, 'Loss_Gain']))
all_series[, 'Drop'] <-as.numeric(as.character(all_series[, 'Drop']))
all_series[, 'Season'] <-as.character(all_series[, 'Season'])
avr_network <- aggregate(AveragePerEpisode ~ Network, data = first, FUN=mean)
all_series <- all_series[-which(all_series$Show == "Coach" & all_series$Season == "2015_16"),]
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
library(shiny)
seasons <- c("2012-2013", "2012-2013", "2014-2015", "2015-2016", "2016-2017")
season_series <- all_series[all_series$Season == head(seasons, 1), ]$Show
get_season <- function(full_season) {
season <- gsub("(20\\d{2})\\-20(\\d{2})", "\\1_\\2", full_season)
}
getTermMatrix <- function(series_name) {
one_series_tweets <- getSeriesTweets(series_name)
myCorpus = Corpus(VectorSource(one_series_tweets))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords,
c(stopwords("SMART"), "thy", "thou", "thee", "the", "and", "but"))
myDTM = TermDocumentMatrix(myCorpus,
control = list(minWordLength = 1))
m = as.matrix(myDTM)
sort(rowSums(m), decreasing = TRUE)
}
#install.packages('devtools')
#library(devtools)
#install_url("https://cran.r-project.org/src/contrib/Archive/slam/slam_0.1-37.tar.gz")
#install.packages("tm")  # for text mining
#install.packages("SnowballC") # for text stemming
#install.packages("wordcloud") # word-cloud generator
#install.packages("RColorBrewer") # color palettes
# Load
library("tm")
library("SnowballC")
library("RColorBrewer")
library("wordcloud")
docs <- Corpus(VectorSource(tweets))
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
set.seed(1234)
a <- wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(16, "Dark2"))
library("tm")
library("SnowballC")
library("RColorBrewer")
library("wordcloud")
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
library(shiny)
library(ggplot2)
library(ggfittext)
library(treemapify)
library(gapminder)
library(gganimate)
library(ggfittext)
library(treemapify)
library(gapminder)
library(gganimate)
theme_set(theme_bw())
series_seasons = c("2012_13", "2013_14", "2014_15", "2015_16", "2016_17")
series_urls <- list("https://docs.google.com/spreadsheets/d/15y9Yi_ypBd8aJSb6h-xvXoAW6Io5X4MNKyxfE-tQ3zk/pubhtml/sheet?hl=en_GB&headers=false&gid=0",
"https://docs.google.com/spreadsheets/d/1GNctAKy1UATkrAdHjC_nvGvtSBivrsUC0hv060I_i64/pubhtml/sheet?hl=en_GB&headers=false&gid=0",
"https://docs.google.com/spreadsheets/d/1nTCF0Nl3ziZeubamDKImnUaITwgA3ar3SipUC-9xt4g/pubhtml/sheet?headers=false&gid=2145367600&range=A1:AC106",
"https://docs.google.com/spreadsheets/d/1D9-DyxF5Qfb2wCmD66htfa5VICjtCOD_sRnM8JC5xw8/pubhtml/sheet?headers=false&gid=2145367600&range=A1:AC106",
"https://docs.google.com/spreadsheets/d/1XydtIiNi0xXzht8ZRROHSm7L12B44Is7wbPg4DqElUw/pubhtml/sheet?headers=false&gid=2145367600&range=A1:AC112"
)
col <- c('Show','Network','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','AveragePerEpisode','Loss_Gain','Drop')
all_series = data.frame()
for(i in 1:length(series_urls)){
URL <- GET(paste(series_urls[i]))
one_series <- readHTMLTable(rawToChar(URL$content))[[1]]
one_series <- one_series[c(-1,-2),]
one_series <- one_series[,c(-1,-4)]
one_series <- one_series[, 1:29]
colnames(one_series) <- col
one_series$Season <- series_seasons[i]
all_series <- rbind(all_series, one_series)
Sys.sleep(1)
}
all_series <- all_series[-which(all_series$Show == "Coach" & all_series$Season == "2015_16"),]
all_series[, 'AveragePerEpisode'] <- as.numeric(as.character(all_series[, 'AveragePerEpisode']))
all_series[, 'Show'] <- as.character(all_series[, 'Show'])
all_series[, 'Network'] <- as.character(all_series[, 'Network'])
all_series[, 'Loss_Gain'] <-as.numeric(as.character(all_series[, 'Loss_Gain']))
all_series[, 'Drop'] <-as.numeric(as.character(all_series[, 'Drop']))
all_series[, 'Season'] <-as.character(all_series[, 'Season'])
seasons <- c("2012-2013", "2012-2013", "2014-2015", "2015-2016", "2016-2017")
season_series <- all_series[all_series$Season == head(seasons, 1), ]$Show
get_season <- function(full_season) {
season <- gsub("(20\\d{2})\\-20(\\d{2})", "\\1_\\2", full_season)
}
getTermMatrix <- function(series_name) {
one_series_tweets <- getSeriesTweets(series_name)
myCorpus = Corpus(VectorSource(one_series_tweets))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords,
c(stopwords("SMART"), "thy", "thou", "thee", "the", "and", "but"))
myDTM = TermDocumentMatrix(myCorpus,
control = list(minWordLength = 1))
m = as.matrix(myDTM)
sort(rowSums(m), decreasing = TRUE)
}
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
api_key <- "AsZZrtfBJh4UuUoyyyoXxCFkT"
api_secret <- "QEeM4rzjm1Swn5dQiOtnfVWaKyZ2MBjPSEqyvL7dNCHbbmB6Cp"
token <- "234059187-XUVObWftjdrvuKdX3wfHhiTHSsEqDl0DWSnYuYn8"
token_secret <- "hfr4ywoys9Hu5rcvB6p7E7JI4L4hG5lt41dSKJNawDFan"
setup_twitter_oauth(api_key, api_secret, token, token_secret)
getSeriesTweets <- function(name) {
tweets <- twListToDF(
searchTwitter(
paste("#", gsub(" ", "", name), sep = ""),
n=500,
lang="en"))
tweets <- tweets$text
tweets <- gsub("^RT", "", tweets)
tweets <- gsub("@", "", tweets)
tweets <- gsub("#", "", tweets)
tweets <- iconv(gsub("\\n", " ", tweets), to="ASCII", sub="")
tweets
}
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
percent
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
View(all_series)
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
a <- aggregate(AveragePerEpisode ~ Network, data = all_series, FUN=mean)
a
a <- aggregate(AveragePerEpisode ~ Network~Season, data = all_series, FUN=mean)
a <- aggregate(all_series, by=list(Network, Season), FUN=mean)
a <- aggregate(all_series, by=list("Network", "Season"), FUN=mean)
a <- aggregate(all_series, by=list("Network", "Season"), FUN=mean, na.rm = TRUE)
a <- aggregate(all_series, by=list("Network"), FUN=mean, na.rm = TRUE)
a <- aggregate(all_series, by=list("AveragePerEpisode","Network", "Season"), FUN=mean, na.rm = TRUE)
?aggregate
k <- aggregate(AveragePerEpisode ~ Network + Season, data = one_season_series, FUN=mean)
k <- aggregate(AveragePerEpisode ~ Network + Season, data = all_series, FUN=mean)
k
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
install.packages("plotly")
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
getTermMatrix <- function(series_name) {
print(series_name)
print(gsub(" ", "", series_name))
one_series_tweets <- getSeriesTweets(series_name)
myCorpus = Corpus(VectorSource(one_series_tweets))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords,
c(stopwords("SMART"), "thy", "thou", "thee", "the", "and", "but", gsub(" ", "", series_name)))
myDTM = TermDocumentMatrix(myCorpus,
control = list(minWordLength = 1))
m = as.matrix(myDTM)
sort(rowSums(m), decreasing = TRUE)
}
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
getTermMatrix <- function(series_name) {
one_series_tweets <- getSeriesTweets(series_name)
myCorpus = Corpus(VectorSource(one_series_tweets))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords,
c(stopwords("SMART"), "thy", "thou", "thee", "the", "and", "but", gsub(" ", "", series_name)))
myDTM = TermDocumentMatrix(myCorpus,
control = list(minWordLength = 1))
m = as.matrix(myDTM)
sort(rowSums(m), decreasing = TRUE)
}
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
getTermMatrix <- function(series_name) {
print(gsub(" ", "", series_name))
one_series_tweets <- getSeriesTweets(series_name)
myCorpus = Corpus(VectorSource(one_series_tweets))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords,
c(stopwords("SMART"), "thy", "thou", "thee", "the", "and", "but", gsub(" ", "", series_name)))
myDTM = TermDocumentMatrix(myCorpus,
control = list(minWordLength = 1))
m = as.matrix(myDTM)
sort(rowSums(m), decreasing = TRUE)
}
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
tolower("QWE")
getTermMatrix <- function(series_name) {
one_series_tweets <- getSeriesTweets(series_name)
myCorpus = Corpus(VectorSource(one_series_tweets))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords,
c(stopwords("SMART"), "thy", "thou", "thee", "the", "and", "but", tolower(gsub(" ", "", series_name))))
myDTM = TermDocumentMatrix(myCorpus,
control = list(minWordLength = 1))
m = as.matrix(myDTM)
sort(rowSums(m), decreasing = TRUE)
}
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
library(MASS)
library(plyr)
library(ggplot2)
colnames(birthwt) <- c("birthwt.below.2500", "mother.age", "mother.weight",
"race", "mother.smokes", "previous.prem.labor", "hypertension", "uterine.irr",
"physician.visits", "birthwt.grams")
birthwt <- transform(birthwt,
race = as.factor(mapvalues(race, c(1, 2, 3),
c("white","black", "other"))),
mother.smokes = as.factor(mapvalues(mother.smokes,
c(0,1), c("no", "yes"))),
hypertension = as.factor(mapvalues(hypertension,
c(0,1), c("no", "yes"))),
uterine.irr = as.factor(mapvalues(uterine.irr,
c(0,1), c("no", "yes")))
)
runif(100, 1, 1)
runif(100, 1)
runif(100, 1, 0)
runif(100, 0, 1)
firstWeek <- c(8, 10, 9, 11, 8, 7, 9, 10, 9, 9)
secondWeek <- c(5, 7, 5, 6, 7, 5, 4, 6, 5, 6)
t.test(firstWeek, secondWeek, alternative = "greater", paired = TRUE)
t.test(firstWeek, secondWeek, alternative = "greater", paired = TRUE, conf.level = 0.99)
t.test(firstWeek)
t.test(secondWeek)
t.test(firstWeek, secondWeek, alternative = "greater", paired = TRUE, conf.level = 0.99)
grades1 <- c(25,41,41,54,29,50,54,46,54,33,33,54,37,12,29,41)
hist(grades)
grades1 <- c(25,41,41,54,29,50,54,46,54,33,33,54,37,12,29,41)
hist(grades1)
qqline(grades1)
qqnorm(grades1)
shapiro.test(grades1)
t.test(grades1, mu=40,alternative = "two.sided", conf.level = 0.95) # rejected
t.test(grades1, mu=40,alternative = "less", conf.level = 0.95) # rejected
t.test(grades1, mu=50,alternative = "two.sided", conf.level = 0.95) # rejected
t.test(grades1, mu=40,alternative = "two.sided", conf.level = 0.95) # rejected
t.test(grades1, mu=40,alternative = "less", conf.level = 0.95) # rejected
t.test(grades1, mu=50,alternative = "two.sided", conf.level = 0.95) # rejected
t.test(grades1, mu=40,alternative = "less", conf.level = 0.95) # rejected
t.test(grades1, mu=39,alternative = "less", conf.level = 0.95) # rejected
t.test(grades1, mu=30,alternative = "less", conf.level = 0.95) # rejected
t.test(grades1, mu=30,alternative = "greater", conf.level = 0.95) # rejected
grades1 <- c(25,41,41,54,29,50,54,46,54,33,33,54,37,12,29,41)
t.test(grades1, mu=40,alternative = "two.sided", conf.level = 0.95) # rejected
t.test(grades1, mu=40,alternative = "two.sided", conf.level = 0.99) # rejected
t.test(grades1, mu=40,alternative = "less", conf.level = 0.95) # rejected
t.test(grades1, mu=38,alternative = "less", conf.level = 0.95) # rejected
t.test(grades1, mu=38,alternative = "greater", conf.level = 0.95) # rejected
t.test(grades1, mu=38,alternative = "two.sided", conf.level = 0.95) # rejected
t.test(grades1, mu=39.5625,alternative = "two.sided", conf.level = 0.95) # rejected
t.test(grades1, mu=40,alternative = "two.sided", conf.level = 0.95) # rejected
t.test(grades1, mu=40,alternative = "less", conf.level = 0.95) # rejected
t.test(grades1, mu=50,alternative = "two.sided", conf.level = 0.95) # rejected
library(BSDA)
z.test(grades1, mu=40, sigma.x=10)
install.packages("BSDA")
library(BSDA)
z.test(grades1, mu=40, sigma.x=10)
grades2 <- c(55,41,51,54,29,50,64,46,54,33,33,34,37,12,29,41)
shapiro.test(grades1)
shapiro.test(grades2)
var.test(grades1, grades2)
t.test(grades1, grades2, var.equal = TRUE)
data <- data.frame(
k <- grades1,
v <- grades2
)
var.test(data$v,data$k)
grade_pre <- grades1
stCoffee <- c(6, 7, 5, 5, 7, 5, 3, 6, 6, 6)
data <- c(notCoffee, coffee, stCoffee) #verileri birle?tirmek i?in
notCoffee <- c(8, 10, 9, 11, 8, 7, 9, 10, 9, 9)
coffee <- c(5, 7, 5, 6, 7, 5, 4, 6, 5, 6)
var.test(notCoffee, coffee) #sigma 1 sigma 2 ye e?it mi diye test ettim(ortalama de?il variance a bakt?k)
stCoffee <- c(6, 7, 5, 5, 7, 5, 3, 6, 6, 6)
data <- c(notCoffee, coffee, stCoffee) #verileri birle?tirmek i?in
group <- c(rep(1,10), rep(2,10), rep(3,10)) #veri k?melerinin 10 eleman?n? se?mek i?in
rep(1,5)
rep(2,5)
group <- c(rep(1,10), rep(2,10), rep(3,10)) #veri k?melerinin 10 eleman?n? se?mek i?in
result <- aov(data ~ as.factor(group))
summary(result) #Pr(>F) p value'ya e?ittir. at leaast one of the groups is different than others
TukeyHSD(result)
cor.test(x,y)
x <- runif(100, 0, 10)
y <- 2 + 3*x + rnorm(100)
cor.test(x,y)
model <- lm(y~x)
summary(model)
cor.test(x,y)
model <- lm(y~x)
summary(model)
?subset
cars
cars
subset(x=cars, subset = speed > 25)
subset(x=cars, subset = speed > 20)
chickwts
with(chickwts, aggregate(weight, by = list(feed), FUN = mean))
aggregate(weight ~ feed, data=chickwts, FUN=mean)
p1 <- c(8,10,9,11,8,7,9,10,9,9)
p2 <- c(5,7,5,6,7,5,4,6,5,6)
p3 <- c(6,7,5,5,7,5,3,6,6,6)
data1 <- c(p1,p2,p3)
group <- c(rep(1,10),rep(2,10),rep(3,10))
results <- aov(data1~as.factor(group))
results
summary(results) # tabloyu doldur
TukeyHSD(results)
firstWeek <- c(8, 10, 9, 11, 8, 7, 9, 10, 9, 9)
secondWeek <- c(5, 7, 5, 6, 7, 5, 4, 6, 5, 6)
t.test(firstWeek, secondWeek, alternative = "greater", paired = TRUE)
var.test(notCoffee, coffee) #sigma 1 sigma 2 ye e?it mi diye test ettim(ortalama de?il variance a bakt?k)
t.test(notCoffee, coffee, alternative = "greater", var.equal = TRUE) #H0 = reject
t.test(firstWeek, secondWeek, alternative = "greater", paired = TRUE)
var.test(notCoffee, coffee) #sigma 1 sigma 2 ye e?it mi diye test ettim(ortalama de?il variance a bakt?k)
t.test(notCoffee, coffee, alternative = "greater", var.equal = F) #H0 = reject
t.test(notCoffee, coffee, alternative = "greater", var.equal = TRUE) #H0 = reject
t.test(firstWeek, secondWeek, alternative = "greater", paired = TRUE)
t.test(firstWeek, secondWeek, alternative = "greater", paired = TRUE, conf.level = 0.99)
t.test(firstWeek, secondWeek, alternative = "greater", paired = TRUE)
t.test(firstWeek ~ secondWeek, alternative = "greater", paired = TRUE)
t.test(firstWeek ~ secondWeek, alternative = "greater", paired = TRUE)
t.test(firstWeek ~ secondWeek, alternative = "greater", paired = FALSE)
?t.test
,
t.test(firstWeek, secondWeek, alternative = "greater", paired = TRUE)
t.test(notCoffee, coffee, alternative = "greater", var.equal = TRUE) #H0 = reject
var.test(p1,p2)   # p-value = 0.6037 > alpha=0.05 ho not reject, variance lar esit
before <- c(237, 289, 257, 228, 303, 275, 262, 304, 244, 233)
after <- c(194, 240, 230, 186, 265, 222, 242, 281, 240, 212)
t.test(before, after, alternative = "two.sided", paired = TRUE)
t.test(before, alternative = "two.sided", paired = TRUE)
t.test(before, after, alternative = "two.sided", paired = TRUE)
t.test(before, after, alternative = "greater", paired = TRUE)
p1 <- c(8,10,9,11,8,7,9,10,9,9)
p2 <- c(5,7,5,6,7,5,4,6,5,6)
t.test(before, after, alternative = "greater", paired = TRUE)
t.test(before - after)
set.seed(123)
x <- c(1:100)
y <- x + rnorm(100, mean=0, sd=10)
lm(y~x)
summary(lm(y~x))
d <- before - after
t.test(d)
t.test(d, conf.level = 0.95)
t.test(d, conf.level = 0.99)
t.test(d, conf.level = 0.95)
t.test(d, conf.level = 0.99)
t.test(d, conf.level = 0.95)
t.test(d, conf.level = 0.99)
t.test(d, conf.level = 0.95)
t.test(d, conf.level = 0.99)
t.test(before)
t.test(after)
t.test(before)
t.test(after)
t.test(before, after, alternative = "greater", paired = TRUE)
t.test(before, after, alternative = "greater", paired = F)
x1 <- runif(100, 0, 10)
summary(lm(y~x))
summary(lm(y~x~x1))
summary(lm(y~x))
t.test(before, after, alternative = "two.sided", paired = TRUE)
summary(lm(y~x+x1))
summary(lm(y~x))
summary(lm(y~x+x1))
summary(lm(y~x))
summary(lm(y~x+x1))
summary(lm(y~x+x1))
summary(lm(y~x))
summary(lm(y~x+x1))
library(XML)
library(httr)
series_seasons = c("2012_13", "2013_14", "2014_15", "2015_16", "2016_17")
series_urls <- list("https://docs.google.com/spreadsheets/d/15y9Yi_ypBd8aJSb6h-xvXoAW6Io5X4MNKyxfE-tQ3zk/pubhtml/sheet?hl=en_GB&headers=false&gid=0",
"https://docs.google.com/spreadsheets/d/1GNctAKy1UATkrAdHjC_nvGvtSBivrsUC0hv060I_i64/pubhtml/sheet?hl=en_GB&headers=false&gid=0",
"https://docs.google.com/spreadsheets/d/1nTCF0Nl3ziZeubamDKImnUaITwgA3ar3SipUC-9xt4g/pubhtml/sheet?headers=false&gid=2145367600&range=A1:AC106",
"https://docs.google.com/spreadsheets/d/1D9-DyxF5Qfb2wCmD66htfa5VICjtCOD_sRnM8JC5xw8/pubhtml/sheet?headers=false&gid=2145367600&range=A1:AC106",
"https://docs.google.com/spreadsheets/d/1XydtIiNi0xXzht8ZRROHSm7L12B44Is7wbPg4DqElUw/pubhtml/sheet?headers=false&gid=2145367600&range=A1:AC112"
)
col <- c('Show','Network','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','AveragePerEpisode','Loss_Gain','Drop')
all_series = data.frame()
for(i in 1:length(series_urls)){
URL <- GET(paste(series_urls[i]))
one_series <- readHTMLTable(rawToChar(URL$content))[[1]]
one_series <- one_series[c(-1,-2),]
one_series <- one_series[,c(-1,-4)]
one_series <- one_series[, 1:29]
colnames(one_series) <- col
one_series$Season <- series_seasons[i]
all_series <- rbind(all_series, one_series)
Sys.sleep(1)
}
all_series <- all_series[-which(all_series$Show == "Coach" & all_series$Season == "2015_16"),]
all_series[, 'AveragePerEpisode'] <- as.numeric(as.character(all_series[, 'AveragePerEpisode']))
all_series[, 'Show'] <- as.character(all_series[, 'Show'])
all_series[, 'Network'] <- as.character(all_series[, 'Network'])
all_series[, 'Loss_Gain'] <-as.numeric(as.character(all_series[, 'Loss_Gain']))
all_series[, 'Drop'] <-as.numeric(as.character(all_series[, 'Drop']))
all_series[, 'Season'] <-as.character(all_series[, 'Season'])
# for(i in 1:length(all_series$Show)){
#   for(j in 3:26){
#     all_series[i, j] <- as.double(as.character(all_series[i, j]))
#   }
#
# }
shiny::runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
api_key <- "AsZZrtfBJh4UuUoyyyoXxCFkT"
api_secret <- "QEeM4rzjm1Swn5dQiOtnfVWaKyZ2MBjPSEqyvL7dNCHbbmB6Cp"
token <- "234059187-XUVObWftjdrvuKdX3wfHhiTHSsEqDl0DWSnYuYn8"
token_secret <- "hfr4ywoys9Hu5rcvB6p7E7JI4L4hG5lt41dSKJNawDFan"
setup_twitter_oauth(api_key, api_secret, token, token_secret)
getSeriesTweets <- function(name) {
tweets <- twListToDF(
searchTwitter(
paste("#", gsub(" ", "", name), sep = ""),
n=500,
lang="en"))
tweets <- tweets$text
tweets <- gsub("^RT", "", tweets)
tweets <- gsub("@", "", tweets)
tweets <- gsub("#", "", tweets)
tweets <- iconv(gsub("\\n", " ", tweets), to="ASCII", sub="")
tweets
}
library(twitteR)
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
library("tm")
library("tm")
library("SnowballC")
library("RColorBrewer")
library("wordcloud")
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
View(all_series)
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
runApp('/media/mersanuzun/Data/lectures/web_mining_with_R/final_project/web_mining_project/shiny')
